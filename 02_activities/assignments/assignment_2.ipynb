{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Creative Text Generation with Recurrent Neural Networks (RNNs)\n",
        "\n",
        "In this assignment, you'll build upon your understanding of RNNs and Keras to develop a word-level text generation model.  Your goal is to train a model that learns the stylistic nuances of a chosen corpus and generates new, original text segments that echo the source material's essence.\n",
        "\n",
        "**Datasets**\n",
        "\n",
        "We've provided several intriguing text corpora to get you started:\n",
        "\n",
        "*   Mark Twain\n",
        "*   Charles Dickens\n",
        "*   William Shakespeare\n",
        "\n",
        "**Feel free to explore!**  If you have a particular passion for another author, genre, or a specific text, you're encouraged to use your own dataset of raw text."
      ],
      "metadata": {
        "collapsed": false,
        "id": "7c6788aef474ca12"
      },
      "id": "7c6788aef474ca12"
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Default GPU Device: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "# Check if we have a GPU available\n",
        "import tensorflow as tf\n",
        "if tf.test.gpu_device_name():\n",
        "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
        "else:\n",
        "    print(\"No GPU available. If you're on Colab, go to Runtime > Change runtime and select a GPU hardware accelerator.\")"
      ],
      "metadata": {
        "id": "2d0bfedcfe52aedc",
        "outputId": "1aedd3b6-943c-43c8-f926-20a5fc71806d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2d0bfedcfe52aedc",
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Choose a book to download:\n",
            "1: Charles Dickens\n",
            "2: Mark Twain\n",
            "3: William Shakespeare\n",
            "Enter the number corresponding to your choice (1, 2, or 3): 2\n",
            "Downloaded mark_twain.txt successfully!\n",
            "Dataset loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "def download_file(book_name):\n",
        "    base_url = \"https://raw.githubusercontent.com/UofT-DSI/deep_learning/refs/heads/main/02_activities/assignments/downloaded_books/\"\n",
        "    file_url = base_url + book_name\n",
        "    local_filename = book_name\n",
        "\n",
        "    response = requests.get(file_url)\n",
        "    if response.status_code == 200:\n",
        "        with open(local_filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(response.text)\n",
        "        print(f\"Downloaded {book_name} successfully!\")\n",
        "        return local_filename\n",
        "    else:\n",
        "        raise ValueError(\"Failed to download the file. Please check the filename and try again.\")\n",
        "\n",
        "def load_dataset(file_path, fraction=1.0):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        raw_text = f.read()\n",
        "    return raw_text[:int(fraction * len(raw_text))]\n",
        "\n",
        "# Prompt user to select a book\n",
        "title_options = {\n",
        "    \"1\": \"charles_dickens.txt\",\n",
        "    \"2\": \"mark_twain.txt\",\n",
        "    \"3\": \"shakespeare.txt\"\n",
        "}\n",
        "\n",
        "print(\"Choose a book to download:\")\n",
        "print(\"1: Charles Dickens\")\n",
        "print(\"2: Mark Twain\")\n",
        "print(\"3: William Shakespeare\")\n",
        "\n",
        "choice = None\n",
        "while choice not in title_options:\n",
        "    choice = input(\"Enter the number corresponding to your choice (1, 2, or 3): \").strip()\n",
        "    if choice not in title_options:\n",
        "        print(\"Invalid choice. Please enter 1, 2, or 3.\")\n",
        "\n",
        "selected_book = title_options[choice]\n",
        "file_path = download_file(selected_book)\n",
        "\n",
        "# Load chosen dataset\n",
        "fraction = 0.1  # Adjust fraction if running out of memory\n",
        "text = load_dataset(file_path, fraction=fraction)\n",
        "\n",
        "print(\"Dataset loaded successfully!\")"
      ],
      "metadata": {
        "id": "9c28c497f620b775",
        "outputId": "44e447bd-b8af-4671-eb8d-061f36eebb03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "9c28c497f620b775",
      "execution_count": 11
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data Preparation (10 Marks)\n",
        "\n",
        "Before we can begin training an RNN model, we need to prepare the dataset. This involves cleaning the text, tokenizing words, and creating sequences the model can be trained on.\n",
        "\n",
        "## 1.1 Data Exploration (3 Marks)\n",
        "\n",
        "Print the first 1000 characters of the dataset. Report the dataset's size and the number of unique characters it contains."
      ],
      "metadata": {
        "collapsed": false,
        "id": "dab51c764031e606"
      },
      "id": "dab51c764031e606"
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution\n",
        "\n",
        "# First 1000 characters of dataset\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "id": "BunkZmdkl0Wn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc6317d9-e357-42c1-878a-5df9b902900d"
      },
      "id": "BunkZmdkl0Wn",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The Project Gutenberg EBook of The Prince and The Pauper, Complete by\n",
            "Mark Twain (Samuel Clemens)\n",
            "\n",
            "This eBook is for the use of anyone anywhere at no cost and with almost\n",
            "no restrictions whatsoever. You may copy it, give it away or re-use\n",
            "it under the terms of the Project Gutenberg License included with this\n",
            "eBook or online at www.gutenberg.org\n",
            "\n",
            "Title: The Prince and The Pauper, Complete\n",
            "\n",
            "Author: Mark Twain (Samuel Clemens)\n",
            "\n",
            "Release Date: August 20, 2006 [EBook #1837]\n",
            "Last Updated: February 19, 2018\n",
            "\n",
            "Language: English\n",
            "\n",
            "Character set encoding: UTF-8\n",
            "\n",
            "*** START OF THIS PROJECT GUTENBERG EBOOK PRINCE AND THE PAUPER ***\n",
            "\n",
            "Produced by David Widger. The earliest PG edition was prepared by Les\n",
            "Bowler\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "THE PRINCE AND THE PAUPER\n",
            "\n",
            "by Mark Twain\n",
            "\n",
            "The Great Seal\n",
            "\n",
            "I will set down a tale as it was told to me by one who had it of his\n",
            "father, which latter had it of HIS father, this last having in like\n",
            "manner had it of HIS father--and so on, back and still back, three\n",
            "hundred years and more, the fat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset's size\n",
        "print('The dataset contains {} characters.'.format(len(text)))"
      ],
      "metadata": {
        "id": "XBzaDB-H0blo",
        "outputId": "1eea4e58-cb24-4e02-acd0-f08515b16a97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "XBzaDB-H0blo",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dataset contains 1267477 characters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print # of unique characters\n",
        "unique_chars = sorted(set(text))\n",
        "print('The dataset contains {} unique characters.'.format(len(unique_chars)))"
      ],
      "metadata": {
        "id": "_9EOqz_902Yt",
        "outputId": "cc077b20-e4ea-4891-a5a2-0611ae7089f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "_9EOqz_902Yt",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dataset contains 90 unique characters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Text Pre-Processing (4 Marks)\n",
        "\n",
        "To prepare the dataset for training, we need to clean the text and create a numerical representation the model can interpret. Perform the following pre-processing steps:\n",
        "\n",
        "*   Convert the entire text to lowercase.\n",
        "*   Use the `Tokenizer` class from the `keras.preprocessing.text` module to tokenize the text. You should fit the tokenizer on the text and then convert the text to a sequence of numbers. You can use the `texts_to_sequences` method to do this.\n",
        "\n",
        "**Note**:\n",
        "* You'll need to specify an appropriate size for the vocabulary. The number of words in the list of most common words can serve as a guide - does it seem like a reasonable vocabulary size?\n",
        "* Some of the words will be excluded from the vocabulary, as they don't appear often enough. It's important to provide a value for `oov_token` when creating the Tokenizer instance, so that these words can be represented as \"unknown\"."
      ],
      "metadata": {
        "collapsed": false,
        "id": "3ae1639f5ecfe587"
      },
      "id": "3ae1639f5ecfe587"
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "# Solution\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "VOCAB_SIZE = 1000\n",
        "OOV_TOKEN = -1\n",
        "\n",
        "# Convert the entire text to lowercase\n",
        "lowercase_text = text.lower()\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=OOV_TOKEN)\n",
        "tokenizer.fit_on_texts([lowercase_text])\n",
        "sequence_lc = tokenizer.texts_to_sequences([lowercase_text])[0]"
      ],
      "metadata": {
        "id": "4d0d30cd98ea453c"
      },
      "id": "4d0d30cd98ea453c",
      "execution_count": 112
    },
    {
      "cell_type": "markdown",
      "source": [
        "If everything worked, the following line should show you the first 10 words in the vocabulary:"
      ],
      "metadata": {
        "collapsed": false,
        "id": "89d32bb9356f711"
      },
      "id": "89d32bb9356f711"
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(-1, 1), ('the', 2), ('and', 3), ('of', 4), ('a', 5), ('to', 6), ('in', 7), ('was', 8), ('he', 9), ('it', 10)]\n"
          ]
        }
      ],
      "source": [
        "print(list(tokenizer.word_index.items())[:10])"
      ],
      "metadata": {
        "id": "6a7cd547a19feece",
        "outputId": "e0ee0044-1bf2-414c-d6f4-1206b3aab310",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "6a7cd547a19feece",
      "execution_count": 113
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Sequence Generation (3 Marks)\n",
        "\n",
        "Now that the text has been tokenized, we need to create sequences the model can be trained on. There are two parts to this:\n",
        "\n",
        "*   Use the `texts_to_sequences` method from the tokenizer to convert the text to a list of sequences of numbers.\n",
        "*   Generate the training sequences. Each training sequence should contain `SEQ_LENGTH` token IDs from the text. The target token for each sequence should be the word that follows the sequence in the text."
      ],
      "metadata": {
        "collapsed": false,
        "id": "da504e4bc6617613"
      },
      "id": "da504e4bc6617613"
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "# Solution\n",
        "SEQ_LENGTH = 3\n",
        "\n",
        "# Convert the text to a list of sequences of numbers\n",
        "sequence = tokenizer.texts_to_sequences([text])[0]\n",
        "\n",
        "# Generate the training sequences\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in range(SEQ_LENGTH, len(sequence)):\n",
        "  input_sequence = sequence[i-SEQ_LENGTH:i]\n",
        "  output_token = sequence[i]\n",
        "\n",
        "  X.append(input_sequence)\n",
        "  y.append(output_token)"
      ],
      "metadata": {
        "id": "4ff5fc8d0273709c"
      },
      "id": "4ff5fc8d0273709c",
      "execution_count": 73
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assuming your sequences are stored in `X` and the corresponding targets in `y`, the following line should print the first training sequence and its target:"
      ],
      "metadata": {
        "collapsed": false,
        "id": "3b6bdc0deb930df1"
      },
      "id": "3b6bdc0deb930df1"
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence: [2, 163, 151]\n",
            "Target: 929\n",
            "Translated back to words: ['the', 'project', 'gutenberg'] -> ebook\n"
          ]
        }
      ],
      "source": [
        "print(f'Sequence: {X[0]}\\nTarget: {y[0]}')\n",
        "print(f'Translated back to words: {[tokenizer.index_word[i] for i in X[0]]} -> {tokenizer.index_word[y[0]]}')"
      ],
      "metadata": {
        "id": "a495cab04001ce92",
        "outputId": "e0015cca-275c-4bc1-d296-c143ab4eb066",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "a495cab04001ce92",
      "execution_count": 74
    },
    {
      "cell_type": "markdown",
      "source": [
        "And the following code will transform y into a one-hot encoded matrix, and split everything into training and validation sets:"
      ],
      "metadata": {
        "collapsed": false,
        "id": "d5bb2c55da17aaa0"
      },
      "id": "d5bb2c55da17aaa0"
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (141202, 3)\n",
            "y_train shape: (141202, 1000)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "# Convert X and y to numpy arrays\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# One last thing: let's drop any examples where the target is the OOV token - we don't want our model to predict that (boring!)\n",
        "mask = y != tokenizer.word_index[OOV_TOKEN]\n",
        "X = X[mask]\n",
        "y = y[mask]\n",
        "\n",
        "# One-hot encode the target token\n",
        "y = to_categorical(y, num_classes=VOCAB_SIZE)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f'X_train shape: {X_train.shape}')\n",
        "print(f'y_train shape: {y_train.shape}')"
      ],
      "metadata": {
        "id": "3a929b2e6c2cc921",
        "outputId": "9498400f-09d9-4746-ae27-2ff1177f2a60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "3a929b2e6c2cc921",
      "execution_count": 75
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Model Development (10 Marks)\n",
        "\n",
        "With the dataset prepared, it's time to develop the RNN model. You'll need to define the architecture of the model, compile it, and prepare it for training.\n",
        "\n",
        "## 2.1 Model Architecture (4 Marks)\n",
        "\n",
        "Define the architecture of your RNN model. You can design it however you like, but there are a few features that it's important to include:\n",
        "\n",
        "*   An embedding layer that learns a dense representation of the input tokens. You'll need to specify the input dimension (the size of the vocabulary) and the output dimension (the size of the dense representation). Remember, you can look at the documentation [here](https://keras.io/api/layers/core_layers/embedding/).\n",
        "*   At least one recurrent layer. We have learned how to use LSTM layers in class, but you can use other types of recurrent layers if you prefer. You can find the documentation [here](https://keras.io/api/layers/recurrent_layers/lstm/).\n",
        "*   A dense layer with a softmax activation function. This layer will output a probability distribution over the vocabulary, so that the model can make predictions about the next token."
      ],
      "metadata": {
        "collapsed": false,
        "id": "b6e4161897210434"
      },
      "id": "b6e4161897210434"
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_11\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_11\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_11 (\u001b[38;5;33mEmbedding\u001b[0m)             │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv1d_12 (\u001b[38;5;33mConv1D\u001b[0m)                   │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d_12 (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_11 (\u001b[38;5;33mLSTM\u001b[0m)                       │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                     │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)             │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv1d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                   │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                       │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Embedding, LSTM\n",
        "\n",
        "EMBEDDING_DIM = 50\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(VOCAB_SIZE, EMBEDDING_DIM, trainable=True),\n",
        "    Conv1D(32, 3, activation='relu', padding='same'),\n",
        "    MaxPooling1D(3),\n",
        "    LSTM(100),\n",
        "    Dense(VOCAB_SIZE, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "9fdfaad93818fc8d",
        "outputId": "f4196503-c396-41a1-e870-189fa0984e35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "id": "9fdfaad93818fc8d",
      "execution_count": 103
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Model Compilation (3 Marks)\n",
        "\n",
        "Compile the model with an appropriate loss function and optimizer. You might also want to track additional metrics, such as accuracy.\n",
        "\n",
        "Give a short explanation of your choice of loss function and optimizer:\n",
        "\n",
        "_your explanation here_"
      ],
      "metadata": {
        "collapsed": false,
        "id": "2fafd2dbb0d589fc"
      },
      "id": "2fafd2dbb0d589fc"
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=Adam(learning_rate=0.01),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# I selected categorical crossentropy for my loss function as it works best with classification problems that use softmax as the model's output layer. For the optimizer, I selected Adam\n",
        "# as the optimizer as it deals with large vocabulary sizes well\n"
      ],
      "metadata": {
        "id": "ae4ca7a12051b1fd"
      },
      "id": "ae4ca7a12051b1fd",
      "execution_count": 104
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Model Training (3 Marks)\n",
        "\n",
        "Train the model on the training data you've prepared.\n",
        "\n",
        "* Train your model for 5 epochs with a batch size of 128. Use the validation data for validation.\n",
        "* Store the training history in a variable called `history`."
      ],
      "metadata": {
        "collapsed": false,
        "id": "c2f0b90a448c4f4b"
      },
      "id": "c2f0b90a448c4f4b"
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m993/993\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.0949 - loss: 5.3940 - val_accuracy: 0.1419 - val_loss: 4.8240\n",
            "Epoch 2/5\n",
            "\u001b[1m993/993\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.1435 - loss: 4.7383 - val_accuracy: 0.1513 - val_loss: 4.6599\n",
            "Epoch 3/5\n",
            "\u001b[1m993/993\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.1594 - loss: 4.5360 - val_accuracy: 0.1558 - val_loss: 4.6119\n",
            "Epoch 4/5\n",
            "\u001b[1m993/993\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.1622 - loss: 4.4389 - val_accuracy: 0.1582 - val_loss: 4.5928\n",
            "Epoch 5/5\n",
            "\u001b[1m993/993\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.1667 - loss: 4.3792 - val_accuracy: 0.1562 - val_loss: 4.5764\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(X_train, y_train, validation_split=0.1,\n",
        "          epochs=5, batch_size=128)"
      ],
      "metadata": {
        "id": "256b1ea138c67ef7",
        "outputId": "1627a0fe-1379-48e7-f2c9-8615da71413c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "256b1ea138c67ef7",
      "execution_count": 105
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the training history to visualize the model's learning progress. Your plot should include the training and validation loss."
      ],
      "metadata": {
        "collapsed": false,
        "id": "195c59bf80d2a2c4"
      },
      "id": "195c59bf80d2a2c4"
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGzCAYAAACPa3XZAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAMA5JREFUeJzt3Xl4VFWC/vH31p6QxQQDBBMWIY0iAirLAzqKDcqjDgP2tCvjI2irrbgwiDb8ZhTQpuNCu3RLK90Ogq1o29qo4450gyOCgIAiIijNEpR9SZEEKpWq+/sjSZHKRgJVp1Lh+3me+9Rdzr33nFzLejnn1i3Ltm1bAAAAhjgSXQEAAHByIXwAAACjCB8AAMAowgcAADCK8AEAAIwifAAAAKMIHwAAwCjCBwAAMIrwAQAAjCJ8AAAAowgfAJplzpw5sixLK1euTHRVACQpwgcAADCK8AEAAIwifACIudWrV+uyyy5TRkaG0tLSNHToUC1btiyqTDAY1LRp01RQUCCfz6e2bdvqggsu0IIFCyJldu7cqbFjxyovL09er1e5ubkaOXKktmzZYrhFAGLJlegKAGhd1q1bp3/5l39RRkaG7r//frndbs2aNUtDhgzR4sWLNXDgQEnS1KlTVVhYqF/84hcaMGCA/H6/Vq5cqVWrVumSSy6RJP37v/+71q1bp7vuuktdunTR7t27tWDBAm3btk1dunRJYCsBnAjLtm070ZUAkDzmzJmjsWPHasWKFerXr1+d7VdeeaXee+89rV+/XqeffrokaceOHerRo4fOOeccLV68WJLUt29f5eXl6Z133qn3PAcPHlRWVpYef/xxTZw4MX4NAmAcwy4AYiYUCumjjz7SqFGjIsFDknJzc3X99dfr008/ld/vlySdcsopWrdunb777rt6j5WSkiKPx6NFixbpwIEDRuoPwAzCB4CY2bNnj8rKytSjR486284880yFw2EVFRVJkh566CEdPHhQP/nJT3T22Wfrvvvu01dffRUp7/V69eijj+r9999X+/btdeGFF+qxxx7Tzp07jbUHQHwQPgAkxIUXXqhNmzZp9uzZ6tWrl55//nmde+65ev755yNlxo8fr40bN6qwsFA+n08PPPCAzjzzTK1evTqBNQdwoggfAGImJydHqamp2rBhQ51t3377rRwOh/Lz8yPrsrOzNXbsWL3yyisqKipS7969NXXq1Kj9unXrpnvvvVcfffSRvv76a5WXl+u3v/1tvJsCII4IHwBixul06tJLL9Vbb70V9XXYXbt2ad68ebrggguUkZEhSdq3b1/UvmlpaerevbsCgYAkqaysTEeOHIkq061bN6Wnp0fKAEhOfNUWwHGZPXu2Pvjggzrrp06dqgULFuiCCy7QHXfcIZfLpVmzZikQCOixxx6LlOvZs6eGDBmi8847T9nZ2Vq5cqVef/113XnnnZKkjRs3aujQobr66qvVs2dPuVwuzZ8/X7t27dK1115rrJ0AYo+v2gJoluqv2jakqKhIe/bs0eTJk7VkyRKFw2ENHDhQ06dP16BBgyLlpk+frrffflsbN25UIBBQ586ddcMNN+i+++6T2+3Wvn37NGXKFC1cuFBFRUVyuVw644wzdO+99+qqq64y0VQAcUL4AAAARnHPBwAAMIrwAQAAjCJ8AAAAo5odPj755BONGDFCHTt2lGVZevPNN6O227atBx98ULm5uUpJSdGwYcMafHwyAAA4+TQ7fJSWlqpPnz6aOXNmvdsfe+wx/e53v9Nzzz2nzz//XG3atNHw4cPrfF8fAACcnE7o2y6WZWn+/PkaNWqUpMpej44dO+ree++N/AplcXGx2rdvrzlz5vDdfAAAENuHjG3evFk7d+7UsGHDIusyMzM1cOBALV26tN7wEQgEop5WGA6HtX//frVt21aWZcWyegAAIE5s29ahQ4fUsWNHORyND6zENHxU/9pk+/bto9a3b9++wV+iLCws1LRp02JZDQAAkCBFRUXKy8trtEzCH68+efJkTZgwIbJcXFysTp06qaioKPIbEAAAoGXz+/3Kz89Xenr6McvGNHx06NBBUuWPSOXm5kbW79q1S3379q13H6/XK6/XW2d9RkYG4QMAgCTTlFsmYvqcj65du6pDhw5auHBhZJ3f79fnn38e9ZsOAADg5NXsno+SkhJ9//33keXNmzdrzZo1ys7OVqdOnTR+/Hj9+te/VkFBgbp27aoHHnhAHTt2jHwjBgAAnNyaHT5Wrlypiy++OLJcfb/GjTfeqDlz5uj+++9XaWmpbr31Vh08eFAXXHCBPvjgA/l8vtjVGgAAJK0W96u2fr9fmZmZKi4u5p4PAEhyoVBIwWAw0dVAjLjdbjmdznq3NefzO+HfdgEAtE4lJSXavn27Wti/cXECLMtSXl6e0tLSTug4hA8AQMyFQiFt375dqampysnJ4aGRrYBt29qzZ4+2b9+ugoKCBntAmoLwAQCIuWAwKNu2lZOTo5SUlERXBzGSk5OjLVu2KBgMnlD4iOlXbQEAqIkej9YlVteT8AEAAIwifAAAAKMIHwAAxEGXLl301FNPJboaLRI3nAIAUGXIkCHq27dvTELDihUr1KZNmxOvVCtE+AAAoIls21YoFJLLdeyPz5ycHAM1Sk4MuwAA4s62bZWVVyRkaupDzsaMGaPFixfr6aeflmVZsixLc+bMkWVZev/993XeeefJ6/Xq008/1aZNmzRy5Ei1b99eaWlp6t+/vz7++OOo49UedrEsS88//7yuvPJKpaamqqCgQG+//XYs/8xJg54PAEDcHQ6G1PPBDxNy7m8eGq5Uz7E/7p5++mlt3LhRvXr10kMPPSRJWrdunSRp0qRJmjFjhk4//XRlZWWpqKhIl19+uaZPny6v16sXX3xRI0aM0IYNG9SpU6cGzzFt2jQ99thjevzxx/X73/9eo0eP1tatW5WdnR2bxiYJej4AAJCUmZkpj8ej1NRUdejQQR06dIg8SOuhhx7SJZdcom7duik7O1t9+vTRbbfdpl69eqmgoEAPP/ywunXrdsyejDFjxui6665T9+7d9Zvf/EYlJSVavny5iea1KPR8AADiLsXt1DcPDU/YuU9Uv379opZLSko0depUvfvuu9qxY4cqKip0+PBhbdu2rdHj9O7dOzLfpk0bZWRkaPfu3Sdcv2RD+AAAxJ1lWU0a+mipan9rZeLEiVqwYIFmzJih7t27KyUlRT//+c9VXl7e6HHcbnfUsmVZCofDMa9vS5e8/yUAABBjHo9HoVDomOWWLFmiMWPG6Morr5RU2ROyZcuWONeu9eCeDwAAqnTp0kWff/65tmzZor179zbYK1FQUKC//e1vWrNmjb788ktdf/31J2UPxvEifAAAUGXixIlyOp3q2bOncnJyGryH44knnlBWVpYGDx6sESNGaPjw4Tr33HMN1zZ5WXZTvwBtiN/vV2ZmpoqLi5WRkZHo6gAAjsORI0e0efNmde3aVT6fL9HVQYw0dl2b8/lNzwcAADCK8AEAAIwifAAAAKMIHwAAwCjCBwAAMIrwAQAAjCJ8AAAAowgfAADAKMIHAAAwivABAEAMdenSRU899VRk2bIsvfnmmw2W37JliyzL0po1a07ovLE6jgn8qi0AAHG0Y8cOZWVlxfSYY8aM0cGDB6NCTX5+vnbs2KFTTz01pueKB8IHAABx1KFDByPncTqdxs51ohh2AQDEn21L5aWJmZrx+6l//OMf1bFjR4XD4aj1I0eO1E033aRNmzZp5MiRat++vdLS0tS/f399/PHHjR6z9rDL8uXLdc4558jn86lfv35avXp1VPlQKKSbb75ZXbt2VUpKinr06KGnn346sn3q1KmaO3eu3nrrLVmWJcuytGjRonqHXRYvXqwBAwbI6/UqNzdXkyZNUkVFRWT7kCFDdPfdd+v+++9Xdna2OnTooKlTpzb573W86PkAAMRfsEz6TcfEnPv//Sh52jSp6FVXXaW77rpL//jHPzR06FBJ0v79+/XBBx/ovffeU0lJiS6//HJNnz5dXq9XL774okaMGKENGzaoU6dOxzx+SUmJ/vVf/1WXXHKJXnrpJW3evFn33HNPVJlwOKy8vDz99a9/Vdu2bfXZZ5/p1ltvVW5urq6++mpNnDhR69evl9/v1wsvvCBJys7O1o8//hh1nB9++EGXX365xowZoxdffFHffvutbrnlFvl8vqiAMXfuXE2YMEGff/65li5dqjFjxuj888/XJZdc0qS/2fEgfAAAUCUrK0uXXXaZ5s2bFwkfr7/+uk499VRdfPHFcjgc6tOnT6T8ww8/rPnz5+vtt9/WnXfeeczjz5s3T+FwWP/zP/8jn8+ns846S9u3b9ftt98eKeN2uzVt2rTIcteuXbV06VK99tpruvrqq5WWlqaUlBQFAoFGh1n+8Ic/KD8/X88884wsy9IZZ5yhH3/8Ub/61a/04IMPyuGoHPzo3bu3pkyZIkkqKCjQM888o4ULFxI+AABJzp1a2QORqHM3w+jRo3XLLbfoD3/4g7xer15++WVde+21cjgcKikp0dSpU/Xuu+9qx44dqqio0OHDh7Vt27YmHXv9+vXq3bu3fD5fZN2gQYPqlJs5c6Zmz56tbdu26fDhwyovL1ffvn2b1Y7169dr0KBBsiwrsu78889XSUmJtm/fHump6d27d9R+ubm52r17d7PO1VyEDwBA/FlWk4c+Em3EiBGybVvvvvuu+vfvr//7v//Tk08+KUmaOHGiFixYoBkzZqh79+5KSUnRz3/+c5WXl8fs/K+++qomTpyo3/72txo0aJDS09P1+OOP6/PPP4/ZOWpyu91Ry5Zl1bnnJdYIHwAA1ODz+fSzn/1ML7/8sr7//nv16NFD5557riRpyZIlGjNmjK688kpJlfdwbNmypcnHPvPMM/XnP/9ZR44cifR+LFu2LKrMkiVLNHjwYN1xxx2RdZs2bYoq4/F4FAqFjnmuN954Q7ZtR3o/lixZovT0dOXl5TW5zvHAt10AAKhl9OjRevfddzV79myNHj06sr6goEB/+9vftGbNGn355Ze6/vrrm9VLcP3118uyLN1yyy365ptv9N5772nGjBlRZQoKCrRy5Up9+OGH2rhxox544AGtWLEiqkyXLl301VdfacOGDdq7d6+CwWCdc91xxx0qKirSXXfdpW+//VZvvfWWpkyZogkTJkTu90gUwgcAALX89Kc/VXZ2tjZs2KDrr78+sv6JJ55QVlaWBg8erBEjRmj48OGRXpGmSEtL0//+7/9q7dq1Ouecc/Rf//VfevTRR6PK3HbbbfrZz36ma665RgMHDtS+ffuiekEk6ZZbblGPHj3Ur18/5eTkaMmSJXXOddppp+m9997T8uXL1adPH/3yl7/UzTffrP/+7/9u5l8j9izbbsYXoA3w+/3KzMxUcXGxMjIyEl0dAMBxOHLkiDZv3qyuXbtG3VyJ5NbYdW3O5zc9HwAAwCjCBwAAMIrwAQAAjCJ8AAAAowgfAIC4aWHfacAJitX1JHwAAGLO6XRKUkyf/InEq76e1df3ePGEUwBAzLlcLqWmpmrPnj1yu90Jf6gVTlw4HNaePXuUmpoql+vE4gPhAwAQc5ZlKTc3V5s3b9bWrVsTXR3EiMPhUKdOnaJ+rO54ED4AAHHh8XhUUFDA0Esr4vF4YtKLRfgAAMSNw+HgCaeog0E4AABgFOEDAAAYRfgAAABGET4AAIBRhA8AAGAU4QMAABhF+AAAAEYRPgAAgFGEDwAAYBThAwAAGEX4AAAARhE+AACAUYQPAABgFOEDAAAYFfPwEQqF9MADD6hr165KSUlRt27d9PDDD8u27VifCgAAJCFXrA/46KOP6tlnn9XcuXN11llnaeXKlRo7dqwyMzN19913x/p0AAAgycQ8fHz22WcaOXKkrrjiCklSly5d9Morr2j58uWxPhUAAEhCMR92GTx4sBYuXKiNGzdKkr788kt9+umnuuyyy+otHwgE5Pf7oyYAANB6xbznY9KkSfL7/TrjjDPkdDoVCoU0ffp0jR49ut7yhYWFmjZtWqyrAQAAWqiY93y89tprevnllzVv3jytWrVKc+fO1YwZMzR37tx6y0+ePFnFxcWRqaioKNZVAgAALYhlx/hrKPn5+Zo0aZLGjRsXWffrX/9aL730kr799ttj7u/3+5WZmani4mJlZGTEsmoAACBOmvP5HfOej7KyMjkc0Yd1Op0Kh8OxPhUAAEhCMb/nY8SIEZo+fbo6deqks846S6tXr9YTTzyhm266KdanAgAASSjmwy6HDh3SAw88oPnz52v37t3q2LGjrrvuOj344IPyeDzH3J9hFwAAkk9zPr9jHj5OFOEDAIDkk9B7PgAAABpD+AAAAEYRPgAAgFGEDwAAYBThAwAAGEX4AAAARhE+AACAUYQPAABgFOEDAAAYRfgAAABGET4AAIBRhA8AAGAU4QMAABhF+AAAAEYRPgAAgFGEDwAAYBThAwAAGEX4AAAARhE+AACAUYQPAABgFOEDAAAYRfgAAABGET4AAIBRhA8AAGAU4QMAABhF+AAAAEYRPgAAgFGEDwAAYBThAwAAGEX4AAAARhE+AACAUYQPAABgFOEDAAAYRfgAAABGET4AAIBRhA8AAGAU4QMAABhF+AAAAEYRPgAAgFGEDwAAYBThAwAAGEX4AAAARhE+AACAUYQPAABgFOEDAAAYRfgAAABGET4AAIBRhA8AAGAU4QMAABhF+AAAAEYRPgAAgFGEDwAAYBThAwAAGEX4AAAARhE+AACAUYQPAABgFOEDAAAYRfgAAABGET4AAIBRhA8AAGAU4QMAABhF+AAAAEYRPgAAgFFxCR8//PCD/uM//kNt27ZVSkqKzj77bK1cuTIepwIAAEnGFesDHjhwQOeff74uvvhivf/++8rJydF3332nrKysWJ8KAAAkoZiHj0cffVT5+fl64YUXIuu6du0a69MAAIAkFfNhl7ffflv9+vXTVVddpXbt2umcc87Rn/70pwbLBwIB+f3+qAkAALReMQ8f//znP/Xss8+qoKBAH374oW6//Xbdfffdmjt3br3lCwsLlZmZGZny8/NjXSUAANCCWLZt27E8oMfjUb9+/fTZZ59F1t19991asWKFli5dWqd8IBBQIBCILPv9fuXn56u4uFgZGRmxrBoAAIgTv9+vzMzMJn1+x7znIzc3Vz179oxad+aZZ2rbtm31lvd6vcrIyIiaAABA6xXz8HH++edrw4YNUes2btyozp07x/pUAAAgCcU8fPznf/6nli1bpt/85jf6/vvvNW/ePP3xj3/UuHHjYn0qAACQhGIePvr376/58+frlVdeUa9evfTwww/rqaee0ujRo2N9KgAAkIRifsPpiWrODSsAAKBlSOgNpwAAAI0hfAAAAKMIHwAAwCjCBwAAMIrwAQAAjCJ8AAAAowgfAADAKMIHAAAwivABAACMInwAAACjCB8AAMAowgcAADCK8AEAAIwifAAAAKMIHwAAwCjCBwAAMIrwAQAAjCJ8AAAAowgfAADAKMIHAAAwivABAACMInwAAACjCB8AAMCokyZ82LatcNhOdDUAADjpuRJdAVMOlAU1qHChOmWnqnPbVHXKblP52jZVnbNTlZeVKo/rpMliAAAkzEkTPrbuK1WgIqzvdpfou90ldbY7LCk3M0Wd2x4NJ5Gg0jZVGT53AmoNAEDrY9m23aLGIvx+vzIzM1VcXKyMjIyYHbciFNaPB49o6/5Sbd1Xpm37y7R139H5svJQo/tnpbrVqW0bdY70nKSqc9vK3pN26V5ZlhWzugIAkGya8/l90oSPxti2rb0l5dpWFUxqhpNt+8u0t6S80f19boc6ZR8dyqkZTvKyUuR2MpwDAGjdmvP5fdIMuzTGsizlpHuVk+7VeZ2z62wvCVRo276yo+Fkf5m27SvT1v2l+uHAYR0JhrVxV4k27qp/OKfjKSlR95l0zq6616RtG6V5uQQAgJMLPR8nKBgK64cDh6sCSd1wciQYbnT/tm08kZteo4Z12qYqJ43hHABAcqDnwyC306Eup7ZRl1PbSMqJ2mbbtvYcCmjr/qqhnH2lR+f3l2l/abn2VU2rtx2sc+xUj7NqOKc6kBwNJ6edkiIXwzkAgCREz0cCHToSrHF/SVnUPSc7ig+rsceSOB2WTosM50R/fbhz21SlesiVAABz6PlIEuk+t3qdlqlep2XW2VZeEdb2AzWGcGqEk237yxSoCGvb/sr5+pya5q11f8nRcNK2jYfhHABAwhA+WiiPy6HTc9J0ek5anW3hsK3dhwKVXxWO3F9ydFjnYFlQe0sC2lsS0BdbD9TZv43HWef+ks5VwSQ308dwDgAgrk6eYZdwSDp8QPKkSS6v1Ir/5V98OBi54bXyXpPK+W37yrTDf0SNXXGXw1JeVkq9zzTplJ2qFI/TXEMAAEmDYZf6FG+Xnu5dOe9wS970hidPmuTNqLEureo1o8b2qm2OlvdhnJni1tl5mTo7r+5wzpFgSNsPHK73mSZFBw6rvCKsLfvKtGVf/cM57dK9de4vqQ4nWaluhnMAAMd08oSP8hrP4AgHpcP7K6cT5W5TK6BUhZSaAaXBkFNj3p1ipDfG53aqe7s0dW9X/3DOTv+R6JtfI/eclMp/pEK7DwW0+1BAK7bUHc5J97rq3F9Sfc9JbmaKnA6CCQDgZBp2kSqHXspLpMChqqlECvhrLB+q2u6vVaZ63l+5/Yi/MsDEkuVspBcmvek9Md50yRmf36E5WFZeI5BEh5Od/iON7ut2WsrPSq33mSb52anyuVteDxIAoOl4vLoJFYGjgSQqoBySyg9FLzcYcqrmFeNL4Eo58Z4Yb7rkadPk3pgjwZCKqr4yXPPm1237y7R9/2GVhxp/2FqHDF8kmNR+pskpqZ5Y/FUAAHFE+Egm4bAULK2/h6Xmcp1emHpCTkXjvQ/NZ8WgJyZDIXcb7SyzK38rp9YTYLfuK9OhIxWN1iLD54p8I6dT21SdkuJWG69LaV6X2nhdauN1RuarX1PdTjkY5gEAY7jhNJk4HEc/qJV7YscKBWv1uDQ2jOSvvxemuifHDkmyq5b9J1Qtp6TTnB6d5k3X4Jq9LNnpsnPTVe5so4Nhn/YHPdpT7tGOI25tL3Npa4mlolKXDgVStPeHFG35IUWl8slW074K3MbjrAonlQGljadmYHEpzeusFWKq1nlcdcJNitvJzbQAECOEj9bE6ZZSsyunE2HbUvBwIz0x9Ux1Qk5V0AmWVh4zVC6V7aucarAkeSW1r5rOrF0Xb93qheRUyHJWvsqpCjlUYTtUIaeCduV85XqnQgGHKgJOhVS5PWRXlq/er+ZrUE7ttZ3aqRr7V+0XlkMOp1sOl1tOp0tOl1sul1tOt1tul1sut1tut0cet1tuj0cet0dej0cej0c+b9Wrxyuf1yOfr3K75XBJUZPz6LzTHb3scFXeF+TgGSwAkh/hA3VZluRJrZzS25/YsUIVR4NLee1el3qGlhoMOX4pXDk841RITjvUQN2rpngJVU3lcTxHI2zLIduKDiqW01UjyDgbDjTHtVy9zh2DY9SYtxxV9xNZVfNVy5ajgXVqYjnr6Hxj5aLK0qMFmEb4QHw5XVLKKZXTibDtypt8y0sqh5fCFVVTqMZ8fcvBJpSpvXx0nR2qULAiqIpguYLBoCqCQVVUBBWqnkJBhSsqFA5VKBwKyg5VyA5XTgpVHsuyQ7LCIVl2hRx2ZX+N2wrJqbBcVf03LoWjX636b9C17LAsu1xq/P5dNFdUIKkvpDiqgm0sy9UISo2GsFiXq1HHRoNZQ8esZ7lm2+tdX084jAp/DezT4P5WA8dqYr0arIsjRm1prC61j9XctrSOwEz4QHKwLMntq5xMnlaSp2qKhVDYVml5hUoDlVNJIFT1WmvdkaDKygM6ciSgI4GgDgcCOhII6EigXIHygALlQQXKy2WHgjVCTI0wY9UKM1Hbw3I1EH7cVcs+p60Upy2fy5bPacvnqHz1OMLyOipfPVZYHkdYbuvo5Kpx7OoeKocqw1dUuLPDlV/yssOS7KrlcGXIrLOuieVORCyOARjVWGBpQsBr2126+cOE1Z7wARjkdFjK8LmV4YvNs1iCoXCN8BKqFWKq5stDdcJOaXnNwHN0v4rqn1JuYFTreFiWqm7irboB2OOS1+WQx+Wo8epscNnbxHIepyWvy6p8dVryOiWP05JDtYNKjdc66+orVxVKmlTOrlqur1x9ocmuZ92JlKtRtqnlGm1LQ3+vev5+deZrH7+h+ep9av2NG92ndhvruZYntI/dSD3rm9cx2lZrPiZqnus4nOi9gSeI8AEkMbfToVNSPTF5Fopt2wpUhKMDSXl0r0xpdU9Ned3gUjMEVZep/v94SdU2KXDijW4mj7N20GkoyDjkcTnrLedtKPg4HfK6HVWvVcseR2S91+mMbOer34hodjA7njDXQOCqLudM7POTCB8AJEmWZcnndsrndqpt3afvN5tt2zocDEUHkkCFysorVF4RVqBqKo96DR1j+ej6musq50OR+ZrKQ2GVh8IqMZ97oridVgOBp5Eg464ZZOpf73HWH5iie44qlz0uBz9z0BJEhkJO3m+vET4AxIVlWUr1uJTqcUnp5s5r27aCIbvR4FIztDSlXGQ5FFYgGI68BkJhBYK111ctV4SjfkE6GLIVDFUkovMnittpNRhwPM6mDYG5nQ65nA65nZZcDksuZ+W+LmflvLtqnctpye2oenVacjkq93VXlXM5rKpjHS1XPU9PUetG+ADQqliWJY/LkseV2H9V2ratirAd3VtTI7iUh0I1Akyt4FJPuaj1dQJUw4Gp/hAUUml5DG/siQOnw4oKJ5XBxYoOKg6H3K7qsFNVtir4VIed6vJuV81j1B+QKkNUjf1qBaSa+7sclf+NNbbd7aSnqSGEDwCIA8uq/Ne+2+mo92F5plSHoKb2/jRlCCwYCisYslVR9RoMhVURrnoN2aoIV20PVy6XV68PhRUMV75WhGwFw9Xl696EGQrbClWFt2RmWZX3ZrnrC0W1wlJjYac6YEV6meo9Xq19GtjuclpK87rUO++UhP1dCB8A0IrVDEFtEhiCGlMdkGqGmnrDTCTINBx2ImGoRiiqDknVYeho2RohKioUHZ0vD4Vr7Fd9vLqBq3q5btuk8opw1XMJW05v0+mnttHfJw5J2PkJHwCAhIrqJUpi1SEqqlenRrA5GnYa2l4dYo6WOzpfN+wEG9leEQ6rPFS3l6k6JOVlpST0b0X4AAAgBo6GKClFzkRXp0VL7pgJAACSDuEDAAAYRfgAAABGET4AAIBRhA8AAGAU4QMAABhF+AAAAEYRPgAAgFGEDwAAYFTcw8cjjzwiy7I0fvz4eJ8KAAAkgbiGjxUrVmjWrFnq3bt3PE8DAACSSNzCR0lJiUaPHq0//elPysrKarBcIBCQ3++PmgAAQOsVt/Axbtw4XXHFFRo2bFij5QoLC5WZmRmZ8vPz41UlAADQAsQlfLz66qtatWqVCgsLj1l28uTJKi4ujkxFRUXxqBIAAGghXLE+YFFRke655x4tWLBAPp/vmOW9Xq+8Xm+sqwEAAFooy7ZtO5YHfPPNN3XllVfK6XRG1oVCIVmWJYfDoUAgELWtNr/fr8zMTBUXFysjIyOWVQMAAHHSnM/vmPd8DB06VGvXro1aN3bsWJ1xxhn61a9+1WjwAAAArV/Mw0d6erp69eoVta5NmzZq27ZtnfUAAODkwxNOAQCAUTHv+ajPokWLTJwGAAAkAXo+AACAUYQPAABgFOEDAAAYRfgAAABGET4AAIBRhA8AAGAU4QMAABhF+AAAAEYRPgAAgFGEDwAAYBThAwAAGEX4AAAARhE+AACAUYQPAABgFOEDAAAYRfgAAABGET4AAIBRhA8AAGAU4QMAABhF+AAAAEYRPgAAgFGEDwAAYBThAwAAGEX4AAAARhE+AACAUYQPAABgFOEDAAAYRfgAAABGET4AAIBRhA8AAGAU4QMAABhF+AAAAEYRPgAAgFGEDwAAYBThAwAAGEX4AAAARhE+AACAUYQPAABgFOEDAAAYRfgAAABGET4AAIBRhA8AAGAU4QMAABhF+AAAAEYRPgAAgFGEDwAAYBThAwAAGEX4AAAARhE+AACAUYQPAABgFOEDAAAYRfgAAABGET4AAIBRhA8AAGAU4QMAABhF+AAAAEYRPgAAgFGEDwAAYBThAwAAGEX4AAAARhE+AACAUYQPAABgFOEDAAAYFfPwUVhYqP79+ys9PV3t2rXTqFGjtGHDhlifBgAAJKmYh4/Fixdr3LhxWrZsmRYsWKBgMKhLL71UpaWlsT4VAABIQpZt23Y8T7Bnzx61a9dOixcv1oUXXlhneyAQUCAQiCz7/X7l5+eruLhYGRkZ8awaAACIEb/fr8zMzCZ9fsf9no/i4mJJUnZ2dr3bCwsLlZmZGZny8/PjXSUAAJBAce35CIfD+rd/+zcdPHhQn376ab1l6PkAACD5NafnwxXPiowbN05ff/11g8FDkrxer7xebzyrAQAAWpC4hY8777xT77zzjj755BPl5eXF6zQAACDJxDx82Latu+66S/Pnz9eiRYvUtWvXWJ8CAAAksZiHj3HjxmnevHl66623lJ6erp07d0qSMjMzlZKSEuvTAQCAJBPzG04ty6p3/QsvvKAxY8Ycc//m3LACAABahoTecBrnx4YAAIAkx2+7AAAAowgfAADAKMIHAAAwivABAACMInwAAACjCB8AAMAowgcAADCK8AEAAIwifAAAAKMIHwAAwCjCBwAAMIrwAQAAjCJ8AAAAowgfAADAKMIHAAAwivABAACMInwAAACjCB8AAMAowgcAADCK8AEAAIwifAAAAKMIHwAAwCjCBwAAMIrwAQAAjCJ8AAAAowgfAADAKMIHAAAwivABAACMInwAAACjCB8AAMAowgcAADCK8AEAAIwifAAAAKMIHwAAwCjCBwAAMIrwAQAAjCJ8AAAAowgfAADAKMIHAAAwivABAACMInwAAACjCB8AAMAowgcAADCK8AEAAIwifAAAAKMIHwAAwCjCBwAAMIrwAQAAjCJ8AAAAowgfAADAKMIHAAAwivABAACMInwAAACjCB8AAMAowgcAADCK8AEAAIwifAAAAKMIHwAAwCjCBwAAMIrwAQAAjCJ8AAAAowgfAADAqLiFj5kzZ6pLly7y+XwaOHCgli9fHq9TAQCAJBKX8PGXv/xFEyZM0JQpU7Rq1Sr16dNHw4cP1+7du+NxOgAAkETiEj6eeOIJ3XLLLRo7dqx69uyp5557TqmpqZo9e3Y8TgcAAJKIK9YHLC8v1xdffKHJkydH1jkcDg0bNkxLly6tUz4QCCgQCESWi4uLJUl+vz/WVQMAAHFS/blt2/Yxy8Y8fOzdu1ehUEjt27ePWt++fXt9++23dcoXFhZq2rRpddbn5+fHumoAACDODh06pMzMzEbLxDx8NNfkyZM1YcKEyHI4HNb+/fvVtm1bWZYV03P5/X7l5+erqKhIGRkZMT12S9Da2ye1/jbSvuTX2ttI+5JfvNpo27YOHTqkjh07HrNszMPHqaeeKqfTqV27dkWt37Vrlzp06FCnvNfrldfrjVp3yimnxLpaUTIyMlrtf1RS62+f1PrbSPuSX2tvI+1LfvFo47F6PKrF/IZTj8ej8847TwsXLoysC4fDWrhwoQYNGhTr0wEAgCQTl2GXCRMm6MYbb1S/fv00YMAAPfXUUyotLdXYsWPjcToAAJBE4hI+rrnmGu3Zs0cPPvigdu7cqb59++qDDz6ocxOqaV6vV1OmTKkzzNNatPb2Sa2/jbQv+bX2NtK+5NcS2mjZTflODAAAQIzw2y4AAMAowgcAADCK8AEAAIwifAAAAKMIHwAAwKhWFz5mzpypLl26yOfzaeDAgVq+fHmj5f/617/qjDPOkM/n09lnn6333nvPUE2PT3PaN2fOHFmWFTX5fD6DtW2eTz75RCNGjFDHjh1lWZbefPPNY+6zaNEinXvuufJ6verevbvmzJkT93qeiOa2cdGiRXWuoWVZ2rlzp5kKN0NhYaH69++v9PR0tWvXTqNGjdKGDRuOuV8yvQePp43J9D589tln1bt378iTLwcNGqT333+/0X2S6fpJzW9jMl2/+jzyyCOyLEvjx49vtJzp69iqwsdf/vIXTZgwQVOmTNGqVavUp08fDR8+XLt37663/GeffabrrrtON998s1avXq1Ro0Zp1KhR+vrrrw3XvGma2z6p8vG5O3bsiExbt241WOPmKS0tVZ8+fTRz5swmld+8ebOuuOIKXXzxxVqzZo3Gjx+vX/ziF/rwww/jXNPj19w2VtuwYUPUdWzXrl2canj8Fi9erHHjxmnZsmVasGCBgsGgLr30UpWWlja4T7K9B4+njVLyvA/z8vL0yCOP6IsvvtDKlSv105/+VCNHjtS6devqLZ9s109qfhul5Ll+ta1YsUKzZs1S7969Gy2XkOtotyIDBgywx40bF1kOhUJ2x44d7cLCwnrLX3311fYVV1wRtW7gwIH2bbfdFtd6Hq/mtu+FF16wMzMzDdUutiTZ8+fPb7TM/fffb5911llR66655hp7+PDhcaxZ7DSljf/4xz9sSfaBAweM1CmWdu/ebUuyFy9e3GCZZHsP1taUNibz+9C2bTsrK8t+/vnn692W7NevWmNtTNbrd+jQIbugoMBesGCBfdFFF9n33HNPg2UTcR1bTc9HeXm5vvjiCw0bNiyyzuFwaNiwYVq6dGm9+yxdujSqvCQNHz68wfKJdDztk6SSkhJ17txZ+fn5x0z3ySaZrt+J6tu3r3Jzc3XJJZdoyZIlia5OkxQXF0uSsrOzGyyT7NewKW2UkvN9GAqF9Oqrr6q0tLTB3+VK9uvXlDZKyXn9xo0bpyuuuKLO9alPIq5jqwkfe/fuVSgUqvMI9/bt2zc4Pr5z585mlU+k42lfjx49NHv2bL311lt66aWXFA6HNXjwYG3fvt1EleOuoevn9/t1+PDhBNUqtnJzc/Xcc8/pjTfe0BtvvKH8/HwNGTJEq1atSnTVGhUOhzV+/Hidf/756tWrV4Plkuk9WFtT25hs78O1a9cqLS1NXq9Xv/zlLzV//nz17Nmz3rLJev2a08Zku36S9Oqrr2rVqlUqLCxsUvlEXMe4/LYLWoZBgwZFpfnBgwfrzDPP1KxZs/Twww8nsGZoqh49eqhHjx6R5cGDB2vTpk168skn9ec//zmBNWvcuHHj9PXXX+vTTz9NdFXipqltTLb3YY8ePbRmzRoVFxfr9ddf14033qjFixc3+OGcjJrTxmS7fkVFRbrnnnu0YMGCFn1jbKsJH6eeeqqcTqd27doVtX7Xrl3q0KFDvft06NChWeUT6XjaV5vb7dY555yj77//Ph5VNK6h65eRkaGUlJQE1Sr+BgwY0KI/1O+880698847+uSTT5SXl9do2WR6D9bUnDbW1tLfhx6PR927d5cknXfeeVqxYoWefvppzZo1q07ZZL1+zWljbS39+n3xxRfavXu3zj333Mi6UCikTz75RM8884wCgYCcTmfUPom4jq1m2MXj8ei8887TwoULI+vC4bAWLlzY4FjeoEGDospL0oIFCxod+0uU42lfbaFQSGvXrlVubm68qmlUMl2/WFqzZk2LvIa2bevOO+/U/Pnz9fe//11du3Y95j7Jdg2Pp421Jdv7MBwOKxAI1Lst2a5fQxprY20t/foNHTpUa9eu1Zo1ayJTv379NHr0aK1Zs6ZO8JASdB3jditrArz66qu21+u158yZY3/zzTf2rbfeap9yyin2zp07bdu27RtuuMGeNGlSpPySJUtsl8tlz5gxw16/fr09ZcoU2+1222vXrk1UExrV3PZNmzbN/vDDD+1NmzbZX3zxhX3ttdfaPp/PXrduXaKa0KhDhw7Zq1evtlevXm1Lsp944gl79erV9tatW23btu1JkybZN9xwQ6T8P//5Tzs1NdW+77777PXr19szZ860nU6n/cEHHySqCcfU3DY++eST9ptvvml/99139tq1a+177rnHdjgc9scff5yoJjTo9ttvtzMzM+1FixbZO3bsiExlZWWRMsn+HjyeNibT+3DSpEn24sWL7c2bN9tfffWVPWnSJNuyLPujjz6ybTv5r59tN7+NyXT9GlL72y4t4Tq2qvBh27b9+9//3u7UqZPt8XjsAQMG2MuWLYtsu+iii+wbb7wxqvxrr71m/+QnP7E9Ho991lln2e+++67hGjdPc9o3fvz4SNn27dvbl19+ub1q1aoE1Lppqr9WWnuqbtONN95oX3TRRXX26du3r+3xeOzTTz/dfuGFF4zXuzma28ZHH33U7tatm+3z+ezs7Gx7yJAh9t///vfEVP4Y6muXpKhrkuzvweNpYzK9D2+66Sa7c+fOtsfjsXNycuyhQ4dGPpRtO/mvn203v43JdP0aUjt8tITraNm2bcevXwUAACBaq7nnAwAAJAfCBwAAMIrwAQAAjCJ8AAAAowgfAADAKMIHAAAwivABAACMInwAAACjCB8AAMAowgcAADCK8AEAAIz6/65PFWcCb7BIAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Solution\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='validation')\n",
        "plt.ylim(0, 10)\n",
        "plt.legend(loc='best')\n",
        "plt.title('Loss');\n"
      ],
      "metadata": {
        "id": "9e8cacec70d8f313",
        "outputId": "8f4cf19a-f2fc-4d44-95d9-9d59537de059",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        }
      },
      "id": "9e8cacec70d8f313",
      "execution_count": 111
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Text Generation (10 Marks)\n",
        "\n",
        "## Task Overview\n",
        "\n",
        "In this task, you will write a function called `generate_text` that uses a trained RNN model to generate new text based on a given seed phrase.\n",
        "\n",
        "Your function **must** follow a structured approach to text generation, where the model predicts one word at a time, adds it to the sequence, and repeats this process until a desired length is reached.\n",
        "\n",
        "## Function Requirements\n",
        "\n",
        "You need to implement a function with the following signature:\n",
        "\n",
        "```python\n",
        "def generate_text(model, tokenizer, seed_text, max_sequence_len, n_words=100):\n",
        "```\n",
        "\n",
        "### **Parameters**\n",
        "- `model`: The trained RNN model that will generate text.\n",
        "- `tokenizer`: The tokenizer used to convert words to numerical sequences.\n",
        "- `seed_text`: The initial text that will be used to start generating words.\n",
        "- `max_sequence_len`: The maximum length of input sequences (same as used in training).\n",
        "- `n_words` (optional, default=100): The number of words to generate.\n",
        "\n",
        "### **Expected Output**\n",
        "- A single **string** containing the generated text.\n",
        "\n",
        "---\n",
        "\n",
        "## **Step-by-Step Instructions**\n",
        "\n",
        "### **1. Tokenize the seed text**\n",
        "Use the tokenizer to convert `seed_text` into a sequence of numbers:\n",
        "\n",
        "```python\n",
        "encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "```\n",
        "\n",
        "### **2. Pad the sequence to match training input length**\n",
        "Ensure that the sequence is the correct length by padding it **at the beginning**:\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "encoded = pad_sequences([encoded], maxlen=max_sequence_len, truncating='pre')\n",
        "```\n",
        "\n",
        "### **3. Predict the next word**\n",
        "Pass the padded sequence to the model to predict the next word.\n",
        "\n",
        "- The model will output a probability distribution over the vocabulary.\n",
        "- Use `np.random.choice` or `np.argmax` to select the most likely word.\n",
        "\n",
        "```python\n",
        "yhat = model.predict(encoded, verbose=0)\n",
        "predicted_word_index = np.argmax(yhat)  # Select the word with the highest probability\n",
        "```\n",
        "\n",
        "### **4. Convert the predicted word index to a word**\n",
        "Find the corresponding word in the tokenizer’s vocabulary:\n",
        "\n",
        "```python\n",
        "out_word = tokenizer.index_word[predicted_word_index]\n",
        "```\n",
        "\n",
        "### **5. Append the new word to the generated text**\n",
        "- Add the predicted word to `seed_text`.\n",
        "- Repeat the process to generate multiple words.\n",
        "\n",
        "```python\n",
        "seed_text += \" \" + out_word\n",
        "```\n",
        "\n",
        "### **6. Repeat Steps 3-5 until `n_words` have been generated**\n",
        "\n",
        "- Each time, remove the oldest word from the input sequence to keep its length constant.\n",
        "- Continue generating words one at a time until reaching `n_words`.\n",
        "\n",
        "---\n",
        "\n",
        "## **Important Notes**\n",
        "- If the generated text doesn’t make much sense, don’t worry! The quality will improve as the model is trained better.\n",
        "- This is a **challenging** task! If you get stuck, ask for help.\n",
        "- The `generate_text` function should return the **full generated text as a single string**.\n",
        "\n",
        "### **Example Usage**\n",
        "After implementing `generate_text`, you should be able to call it like this:\n",
        "\n",
        "```python\n",
        "generated_text = generate_text(model, tokenizer, \"Once upon a time\", max_sequence_len=20, n_words=50)\n",
        "print(generated_text)\n",
        "```\n",
        "\n",
        "This should output a string of 50 words generated by the model, starting with `\"Once upon a time\"`."
      ],
      "metadata": {
        "collapsed": false,
        "id": "3d9ed71305787aed"
      },
      "id": "3d9ed71305787aed"
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def generate_text(model, tokenizer, seed_text, max_sequence_len, n_words=100):\n",
        "\n",
        "  # Tokenize the seed text\n",
        "  encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\n",
        "  # Pad the sequence to match training input length\n",
        "  encoded = pad_sequences([encoded], maxlen=max_sequence_len, truncating='pre')\n",
        "\n",
        "  for i in range(n_words):\n",
        "    # Predict the next word\n",
        "    yhat = model.predict(encoded, verbose=0)\n",
        "    predicted_word_index = np.argmax(yhat)\n",
        "\n",
        "    # Convert the predicted word to the generated text\n",
        "    out_word = tokenizer.index_word[predicted_word_index]\n",
        "\n",
        "    if out_word:\n",
        "      # Append the new word to the generated text\n",
        "      seed_text += \" \" + out_word\n",
        "    else:\n",
        "      break\n",
        "\n",
        "    # Repeat until n_words have been generated\n",
        "    encoded = pad_sequences([encoded[0][1:] + [predicted_word_index]], maxlen=max_sequence_len, truncating='pre')\n",
        "\n",
        "  return seed_text\n",
        "\n"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-02-08T21:38:43.251561Z",
          "start_time": "2024-02-08T21:38:20.349248Z"
        },
        "id": "d73dbf278a1265ef"
      },
      "id": "d73dbf278a1265ef",
      "execution_count": 130
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "np.int64(0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-131-332e9227fdce>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test the text generation function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hamlet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSEQ_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-130-a25a6ce36718>\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(model, tokenizer, seed_text, max_sequence_len, n_words)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Convert the predicted word to the generated text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mout_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredicted_word_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mout_word\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: np.int64(0)"
          ]
        }
      ],
      "source": [
        "# Test the text generation function\n",
        "generate_text(model, tokenizer, 'hamlet', SEQ_LENGTH)\n",
        "\n",
        "# The first time I ran my code I got no error and it worked properly, when I ran it again this happened - I\n",
        "# have no idea what happened, I didn't make any changes to the code"
      ],
      "metadata": {
        "id": "f463b0c3df49e2c",
        "outputId": "294a0802-4ccd-42df-bae3-45327f4bbed2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "id": "f463b0c3df49e2c",
      "execution_count": 131
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Model Refinement (5 Marks)\n",
        "\n",
        "## **Understanding Your Model's Performance**\n",
        "\n",
        "At this stage, you might have noticed that the text generated by your model doesn’t make much sense yet. **This is completely expected!**\n",
        "\n",
        "There are a few reasons why:\n",
        "1. **RNNs have limitations** – While they can generate sequences, they struggle with long-range dependencies in text.\n",
        "2. **Character-by-character generation is outdated** – Modern models like ChatGPT don’t generate text one letter at a time. Instead, they use **tokens**, which represent larger chunks of words, making their outputs much more coherent.\n",
        "3. **Training time and data size** – Our model has been trained on a relatively small dataset for a short period of time, which means it hasn’t learned enough patterns to generate meaningful text.\n",
        "\n",
        "Even though we don’t expect ChatGPT-level performance, this exercise is about **experimentation, not perfection**. Your goal here is to try **at least one** way to refine your model and observe how it affects the output.\n",
        "\n",
        "---\n",
        "\n",
        "## **Refining Your Model**\n",
        "There are many ways to try improving your model. Here are some ideas:\n",
        "\n",
        "✅ **Use pre-trained embeddings**  \n",
        "   Instead of learning word representations from scratch, you can use pre-trained word embeddings. This allows your model to start with a better understanding of word relationships.\n",
        "\n",
        "✅ **Modify the model architecture**  \n",
        "   - Experiment with **more layers** or different numbers of units per layer.  \n",
        "   - Try adding **dropout layers** to prevent overfitting.  \n",
        "   - Consider using **bidirectional RNNs**, which process text in both forward and backward directions.  \n",
        "\n",
        "✅ **Train for longer**  \n",
        "   - Try increasing the number of **epochs** (but be mindful of overfitting).  \n",
        "   - Experiment with different **batch sizes** to see if they affect training stability.  \n",
        "\n",
        "Again, **perfection is NOT the goal here** – we just want to see that you experimented with improving your model! 🚀"
      ],
      "metadata": {
        "collapsed": false,
        "id": "5871d836a0135c41"
      },
      "id": "5871d836a0135c41"
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-18 07:28:23--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.06MB/s    in 2m 41s  \n",
            "\n",
            "2025-03-18 07:31:04 (5.12 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ],
      "source": [
        "!wget https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "id": "dda8b0f845c20862",
        "outputId": "aedbacc9-8b22-4cfe-a954-d65378239ece",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "dda8b0f845c20862",
      "execution_count": 117
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ],
      "source": [
        "# Load the pre-trained embeddings\n",
        "embeddings_index = {}\n",
        "with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(f'Found {len(embeddings_index)} word vectors.')"
      ],
      "metadata": {
        "id": "e8b777220505635",
        "outputId": "af91caa6-55d2-4af6-b024-170e7d6ffad5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "e8b777220505635",
      "execution_count": 118
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "# Create an embedding matrix\n",
        "embedding_matrix = np.zeros((VOCAB_SIZE, 100))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i < VOCAB_SIZE:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "d3e48ff004757cf2"
      },
      "id": "d3e48ff004757cf2",
      "execution_count": 119
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "embedding_layer = Embedding(\n",
        "    VOCAB_SIZE, 100, weights=[embedding_matrix], trainable=False\n",
        ")"
      ],
      "metadata": {
        "id": "e3d21d5dbbbcf9f9"
      },
      "id": "e3d21d5dbbbcf9f9",
      "execution_count": 120
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = 50\n",
        "\n",
        "model = Sequential([\n",
        "    embedding_layer,\n",
        "    Conv1D(32, 3, activation='relu', padding='same'),\n",
        "    MaxPooling1D(3),\n",
        "    LSTM(100),\n",
        "    Dense(VOCAB_SIZE, activation='softmax')\n",
        "])\n",
        "\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=Adam(learning_rate=0.01),\n",
        "    metrics=['accuracy']\n",
        ")\n"
      ],
      "metadata": {
        "id": "f16570310f0f56b"
      },
      "id": "f16570310f0f56b",
      "execution_count": 121
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "np.int64(0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-132-332e9227fdce>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test the text generation function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hamlet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSEQ_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-130-a25a6ce36718>\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(model, tokenizer, seed_text, max_sequence_len, n_words)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Convert the predicted word to the generated text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mout_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredicted_word_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mout_word\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: np.int64(0)"
          ]
        }
      ],
      "source": [
        "# Test the text generation function\n",
        "generate_text(model, tokenizer, 'hamlet', SEQ_LENGTH)\n",
        "\n",
        "# Same thing with this one, getting the same error second time running"
      ],
      "metadata": {
        "id": "ae362e2dd29be2e1",
        "outputId": "ee1a8963-0cc9-4e6b-dd6f-1b9f7284a2ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "id": "ae362e2dd29be2e1",
      "execution_count": 132
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [],
      "metadata": {
        "id": "236cb723e4e5b3fc"
      },
      "id": "236cb723e4e5b3fc",
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}